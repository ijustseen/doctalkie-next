Хорошо, давайте вернемся к концепции **Retrieval-Augmented Generation (RAG)** и как реализовать хранение и поиск данных _индивидуально для каждого ассистента (бота)_.

Основная идея — связать каждый фрагмент загруженной информации (чанк документа) с конкретным `assistant_id`.

**Процесс Индексации (Загрузка и Подготовка Документов):**

Это одноразовый процесс, который происходит, когда вы (или пользователь) загружаете документ _для конкретного ассистента_.

1.  **Загрузка Документа:**

    - Пользователь выбирает ассистента, для которого хочет загрузить документ.
    - Пользователь загружает файл (PDF, DOCX, TXT и т.д.) через интерфейс.
    - Запрос отправляется на специальный бэкенд-эндпоинт, например, `/api/assistants/{assistant_id}/documents` или `/api/documents/process`. Важно, чтобы вместе с файлом передавался `assistant_id`.

2.  **Обработка на Бэкенде (`/api/.../process`):**
    - **Получение Файла и ID:** Бэкенд получает файл и `assistant_id`.
    - **Парсинг Документа:** Извлечение чистого текста из файла. Для PDF могут понадобиться библиотеки типа `pdf-parse`, для DOCX - `mammoth`, и т.д.
    - **Чанкинг (Chunking):** Текст документа разбивается на небольшие, перекрывающиеся (для сохранения контекста на стыках) или неперекрывающиеся фрагменты (чанки). Обычный размер — 100-1000 токенов. Это нужно, потому что:
      - Модели эмбеддингов лучше работают с короткими текстами.
      - Векторный поиск по коротким чанкам точнее.
      - Мы можем передать в LLM только самые релевантные чанки, не превышая лимит контекста LLM.
    - **Генерация Эмбеддингов:** Для _каждого_ чанка текста генерируется векторное представление (эмбеддинг) с помощью выбранной модели (например, модели из Hugging Face `sentence-transformers`, OpenAI `text-embedding-ada-002` и т.д.). **Важно:** Вы должны использовать _одну и ту же_ модель эмбеддингов для индексации и для запросов пользователя позже.
    - **Сохранение в Векторную Базу Данных:** Для _каждого_ чанка в вашу векторную базу данных (например, таблица Supabase с расширением `pgvector`) сохраняется запись, содержащая как минимум:
      - `embedding`: Сгенерированный вектор.
      - `content`: Текст самого чанка.
      - `assistant_id`: **Ключевой момент!** ID ассистента, к которому относится этот чанк.
      - _Опционально:_ `document_id` или `document_name`, `chunk_index` и т.д. для удобства.

**Пример Структуры Таблицы в Supabase (с pgvector):**

Назовем ее, например, `document_chunks`:

| Колонка        | Тип                        | Описание                                                                  |
| :------------- | :------------------------- | :------------------------------------------------------------------------ |
| `id`           | `uuid` (Primary Key)       | Уникальный ID чанка                                                       |
| `assistant_id` | `uuid` (Foreign Key)       | ID ассистента из вашей таблицы `assistants`. **Нужен индекс!**            |
| `content`      | `text`                     | Текстовое содержимое чанка.                                               |
| `embedding`    | `vector(размерность)`      | Векторное представление чанка (размерность зависит от модели эмбеддинга). |
| `metadata`     | `jsonb` (Опционально)      | Доп. информация (имя файла, номер страницы и т.д.).                       |
| `created_at`   | `timestamp with time zone` | Время создания записи.                                                    |

**Важно:** На колонку `embedding` нужно добавить индекс для быстрого векторного поиска (например, `hnsw` или `ivfflat`), а на `assistant_id` — обычный индекс (B-tree) для быстрой фильтрации.

**Процесс Поиска и Ответа (Во время Чата):**

Это происходит каждый раз, когда пользователь отправляет сообщение в чат через эндпоинт `/api/chat/{id}`.

1.  **Получение Запроса:** Бэкенд (`/api/chat/{id}/route.ts`) получает `query` пользователя и `id` ассистента из URL.
2.  **Аутентификация/Авторизация:** Проверяется `apiKey`, как мы уже реализовали.
3.  **Генерация Эмбеддинга Запроса:** Текст `query` пользователя преобразуется в вектор с помощью **той же самой** модели эмбеддингов, что использовалась при индексации.
4.  **Векторный Поиск с Фильтрацией:** Выполняется запрос к векторной базе данных (таблице `document_chunks`):
    - Ищется N векторов (`embedding`), наиболее похожих (например, по косинусному расстоянию) на эмбеддинг запроса пользователя.
    - **Ключевой момент:** В запрос **обязательно** добавляется фильтр `WHERE assistant_id = {id_из_url}`. Это гарантирует, что поиск будет выполняться _только_ по чанкам, относящимся к данному конкретному ассистенту.
5.  **Формирование Контекста:** Текстовое содержимое (`content`) найденных N наиболее релевантных чанков объединяется в один большой текст (контекст).
6.  **Вызов LLM:** Формируется промпт для LLM (например, Groq), включающий:
    - Инструкцию (например, "Ответь на вопрос пользователя, основываясь только на предоставленном контексте. Не используй свои общие знания. Если ответ не найден в контексте, скажи об этом.").
    - Сформированный Контекст.
    - Запрос Пользователя (`query`).
7.  **Получение и Отправка Ответа:** Ответ, сгенерированный LLM, отправляется обратно пользователю через API.

Таким образом, использование колонки `assistant_id` в таблице с чанками и обязательная фильтрация по ней при поиске позволяют эффективно изолировать базы знаний для каждого ассистента в рамках одной векторной базы данных.
